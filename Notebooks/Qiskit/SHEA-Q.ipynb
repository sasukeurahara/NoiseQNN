{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7434765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score\n",
    "import time\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_aer.noise import NoiseModel\n",
    "from qiskit_aer.noise.errors import depolarizing_error, amplitude_damping_error, phase_damping_error, pauli_error\n",
    "from qiskit.quantum_info import Kraus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048c7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "n_qubits = 2\n",
    "noise_prob = 0.3   # Change this to set noise probability (0.0 = noiseless)\n",
    "noise_type = \"AmplitudeDamping\"  # Options: \"Depolarizing\", \"AmplitudeDamping\", \"PhaseDamping\", \"BitFlip\", \"PhaseFlip\"\n",
    "\n",
    "# Data preparation\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification (Setosa vs Versicolor)\n",
    "mask = y < 2\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Use only first 2 features\n",
    "X = X[:, :2]\n",
    "y = y.astype(float)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Torch tensors in float64\n",
    "X_train = torch.tensor(X_train, dtype=torch.float64)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float64)\n",
    "\n",
    "def create_noise_model(noise_type, noise_prob):\n",
    "    \"\"\"Create Qiskit noise model\"\"\"\n",
    "    if noise_prob == 0.0:\n",
    "        return None\n",
    "    \n",
    "    noise_model = NoiseModel()\n",
    "    \n",
    "    if noise_type == \"Depolarizing\":\n",
    "        error = depolarizing_error(noise_prob, 1)\n",
    "    elif noise_type == \"AmplitudeDamping\":\n",
    "        error = amplitude_damping_error(noise_prob)\n",
    "    elif noise_type == \"PhaseDamping\":\n",
    "        error = phase_damping_error(noise_prob)\n",
    "    elif noise_type == \"BitFlip\":\n",
    "        # Create bit flip error using pauli_error\n",
    "        error = pauli_error([('X', noise_prob), ('I', 1 - noise_prob)])\n",
    "    elif noise_type == \"PhaseFlip\":\n",
    "        # Create phase flip error using pauli_error\n",
    "        error = pauli_error([('Z', noise_prob), ('I', 1 - noise_prob)])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Apply noise to all single-qubit gates\n",
    "    noise_model.add_all_qubit_quantum_error(error, ['rx', 'ry', 'rz'])\n",
    "    # Apply noise to CNOT gates (using 2-qubit depolarizing for entangling gates)\n",
    "    if noise_type == \"Depolarizing\":\n",
    "        cnot_error = depolarizing_error(noise_prob, 2)\n",
    "        noise_model.add_all_qubit_quantum_error(cnot_error, ['cx'])\n",
    "    \n",
    "    return noise_model\n",
    "\n",
    "def feature_encoding(circuit, x):\n",
    "    \"\"\"Encode features using RX rotations\"\"\"\n",
    "    for i in range(n_qubits):\n",
    "        circuit.rx(float(x[i]), i)\n",
    "\n",
    "def shallow_HEA(circuit, weights):\n",
    "    \"\"\"Shallow Hardware Efficient Ansatz\"\"\"\n",
    "    # Convert weights to numpy if it's a tensor\n",
    "    if isinstance(weights, torch.Tensor):\n",
    "        weights = weights.detach().cpu().numpy()\n",
    "    \n",
    "    # Parameterized rotations\n",
    "    for i in range(n_qubits):\n",
    "        circuit.rx(float(weights[i, 0]), i)\n",
    "        circuit.rz(float(weights[i, 1]), i)\n",
    "    \n",
    "    # Entangling gate\n",
    "    circuit.cx(0, 1)\n",
    "\n",
    "def create_circuit(weights, x):\n",
    "    \"\"\"Create the complete quantum circuit\"\"\"\n",
    "    circuit = QuantumCircuit(n_qubits)\n",
    "    \n",
    "    # Feature encoding\n",
    "    feature_encoding(circuit, x)\n",
    "    \n",
    "    # Shallow HEA\n",
    "    shallow_HEA(circuit, weights)\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "def get_expectation_value(circuit, noise_model=None, shots=8192):\n",
    "    \"\"\"Calculate expectation value of Pauli-Z on qubit 0\"\"\"\n",
    "    # Create the observable (Pauli-Z on qubit 0)\n",
    "    observable = SparsePauliOp(['Z' + 'I' * (n_qubits - 1)], coeffs=[1.0])\n",
    "    \n",
    "    # Use AerSimulator\n",
    "    if noise_model is not None:\n",
    "        simulator = AerSimulator(noise_model=noise_model)\n",
    "    else:\n",
    "        simulator = AerSimulator(method='statevector')\n",
    "    \n",
    "    # For noisy simulation, use sampling\n",
    "    if noise_model is not None:\n",
    "        # Add measurements to circuit\n",
    "        circuit_copy = circuit.copy()\n",
    "        circuit_copy.measure_all()\n",
    "        \n",
    "        # Transpile and run\n",
    "        transpiled = transpile(circuit_copy, simulator)\n",
    "        job = simulator.run(transpiled, shots=shots)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts()\n",
    "        \n",
    "        # Calculate expectation value from measurement statistics\n",
    "        expectation = 0.0\n",
    "        total_shots = sum(counts.values())\n",
    "        \n",
    "        for bitstring, count in counts.items():\n",
    "            # For Pauli-Z on qubit 0, we look at the rightmost bit (qubit 0)\n",
    "            z_eigenvalue = 1 if bitstring[-1] == '0' else -1\n",
    "            expectation += z_eigenvalue * count / total_shots\n",
    "            \n",
    "    else:\n",
    "        # For noiseless simulation, use statevector\n",
    "        from qiskit.quantum_info import Statevector\n",
    "        transpiled = transpile(circuit, simulator)\n",
    "        job = simulator.run(transpiled)\n",
    "        result = job.result()\n",
    "        statevector = result.get_statevector()\n",
    "        expectation = statevector.expectation_value(observable).real\n",
    "    \n",
    "    return expectation\n",
    "\n",
    "class QiskitQuantumLayer:\n",
    "    \"\"\"Quantum layer using Qiskit\"\"\"\n",
    "    def __init__(self, noise_model):\n",
    "        self.noise_model = noise_model\n",
    "    \n",
    "    def forward(self, weights, x):\n",
    "        \"\"\"Forward pass through quantum circuit\"\"\"\n",
    "        # Convert tensor inputs to numpy\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().cpu().numpy()\n",
    "        \n",
    "        # Create and execute circuit\n",
    "        circuit = create_circuit(weights, x)\n",
    "        expectation = get_expectation_value(circuit, self.noise_model)\n",
    "        \n",
    "        return expectation\n",
    "\n",
    "class QuantumClassifier(torch.nn.Module):\n",
    "    def __init__(self, noise_model):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(0.01 * torch.randn(n_qubits, 2, dtype=torch.float64))\n",
    "        self.quantum_layer = QiskitQuantumLayer(noise_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for xi in x:\n",
    "            # Get expectation value from quantum circuit\n",
    "            expectation = self.quantum_layer.forward(self.weights, xi)\n",
    "            outputs.append(expectation)\n",
    "        \n",
    "        # Convert to tensor and map from [-1,1] to [0,1]\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float64)\n",
    "        return (outputs.reshape(-1, 1) * 0.5 + 0.5)\n",
    "\n",
    "# Custom autograd function for quantum layer\n",
    "class QuantumFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, weights, x, quantum_layer):\n",
    "        ctx.quantum_layer = quantum_layer\n",
    "        ctx.save_for_backward(weights, x)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = []\n",
    "        for xi in x:\n",
    "            expectation = quantum_layer.forward(weights, xi)\n",
    "            outputs.append(expectation)\n",
    "        \n",
    "        return torch.tensor(outputs, dtype=torch.float64)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weights, x = ctx.saved_tensors\n",
    "        quantum_layer = ctx.quantum_layer\n",
    "        \n",
    "        # Parameter-shift rule for gradients\n",
    "        epsilon = np.pi / 2  # For parameter-shift rule\n",
    "        grad_weights = torch.zeros_like(weights)\n",
    "        \n",
    "        for i in range(weights.shape[0]):\n",
    "            for j in range(weights.shape[1]):\n",
    "                grad_param = 0.0\n",
    "                \n",
    "                for k, xi in enumerate(x):\n",
    "                    # Shift parameter forward\n",
    "                    weights_plus = weights.clone()\n",
    "                    weights_plus[i, j] += epsilon\n",
    "                    exp_plus = quantum_layer.forward(weights_plus, xi)\n",
    "                    \n",
    "                    # Shift parameter backward  \n",
    "                    weights_minus = weights.clone()\n",
    "                    weights_minus[i, j] -= epsilon\n",
    "                    exp_minus = quantum_layer.forward(weights_minus, xi)\n",
    "                    \n",
    "                    # Parameter-shift gradient\n",
    "                    grad_param += grad_output[k] * (exp_plus - exp_minus) / 2.0\n",
    "                \n",
    "                grad_weights[i, j] = grad_param\n",
    "        \n",
    "        return grad_weights, None, None\n",
    "\n",
    "class QuantumClassifierWithGrad(torch.nn.Module):\n",
    "    def __init__(self, noise_model):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(0.01 * torch.randn(n_qubits, 2, dtype=torch.float64))\n",
    "        self.quantum_layer = QiskitQuantumLayer(noise_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use custom autograd function\n",
    "        quantum_outputs = QuantumFunction.apply(self.weights, x, self.quantum_layer)\n",
    "        # Map from [-1,1] to [0,1]\n",
    "        return (quantum_outputs.reshape(-1, 1) * 0.5 + 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950896e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Noise type: AmplitudeDamping, Noise probability: 0.3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass with finite differences (since parameter-shift is complex)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Create noise model\n",
    "noise_model = create_noise_model(noise_type, noise_prob)\n",
    "\n",
    "# Create model (use simpler version for faster training)\n",
    "model = QuantumClassifier(noise_model)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Noise type: {noise_type}, Noise probability: {noise_prob}\")\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train.reshape(-1, 1))\n",
    "    \n",
    "    # Backward pass with finite differences (since parameter-shift is complex)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_eval = model(X_train)\n",
    "            acc = ((y_pred_eval.detach().numpy() > 0.5) == y_train.numpy().reshape(-1, 1)).mean()\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: {loss.item():.4f} | Train Acc: {acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75050f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "\n",
      "=== SHEA-P Qiskit Evaluation ===\n",
      "Noise: AmplitudeDamping\n",
      "Noise probability: 0.3\n",
      "MSE: 0.5377\n",
      "RMSE: 0.7333\n",
      "MAE: 0.6060\n",
      "F1-score: 0.5714\n",
      "Inference time per sample: 0.026233 sec\n",
      "No. of qubits: 2\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    preds = model(X_test).detach().numpy()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    inference_time_per_sample = (end_time - start_time) / len(X_test)\n",
    "    \n",
    "    y_true_np = y_test.detach().numpy().flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_true_np, preds.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_np, preds.flatten())\n",
    "    \n",
    "    # F1 score\n",
    "    y_true_int = y_test.detach().numpy().astype(int)\n",
    "    f1 = f1_score(y_true_int, preds_binary.flatten())\n",
    "\n",
    "print(f\"\\n=== SHEA-P Qiskit Evaluation ===\")\n",
    "print(f\"Noise: {noise_type}\")\n",
    "print(f\"Noise probability: {noise_prob}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Inference time per sample: {inference_time_per_sample:.6f} sec\")\n",
    "print(f\"No. of qubits: {n_qubits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a80742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
